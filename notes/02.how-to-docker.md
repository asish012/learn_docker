### Getting a shell inside containers ###
* start new container interactively
    > docker container run -it
* run additional command in existing container
    > docker container exec -it

- create a container
    > docker container create --name webhost -p 80:80 nginx

- get into a container and run bash within that container
    > docker container run -it --name webhost nginx bash
- investigate: see all docker containers (including stopped containers)
    > docker container ls -a
- investigate: what happenes when we run additional command
    > docker container ls -a
    - you will see the COMMAND it runs is "bash", and for usual nginx server it is simply [nginx -g daemon ...]
    - container runs as long as the COMMAND runs. so when we exit the "bash" the container exits as well.

- run ubuntu container
    > docker container run -it --name ubuntu ubuntu
    > apt update
    > apt install -y curl
    > curl google.com

- start existing container
    > docker container start -ai webhost
* note: we can't run a COMMAND with docker start

- start bash or other COMMAND with an existing container (container must be started before)
    > docker container start webhost
    > docker container exec -it webhost bash
* docker exec command actually creates a new process out of the container and that process keeps running even if we exit the container (unlike the COMMAND option of docker run). docker container ls would show the running process.
    > docker container ls

- auto remove containers once they exit [--rm]
> docker container run --rm -it centos:7 bash
> docker container run --rm -it ubuntu:14.04 bash

- pull image
    > docker pull alpine
- start alpine and start bash
* alpine is so small that it doesn't have "bash". "sh" is included though. so we can run "sh" and later install "bash" if needed.
    > docker container run -it --name alpine alpine sh
    > apk add bash

### Docker networking ###
* port assignment with [-p]
* docker container port [container]
* how containers talk to each other

- create container
    > docker container run -p 80:80 --name webhost -d nginx
    > docker container port webhost
    > docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost
    * notice that the docker container network is not the same as your local network [ifconfig]

- docker works very similar fashion like aws-subnets (virtual network). when we create a container it is by default associated with a default sub-network. we can create our own sub-networks.
- all the containers within a sub-network can talk to each other out of the box without associating/opening any ports. but if inter-subnet communication is required, then those communication has to happen through the associated ports to those containers subnets.

**[ Docker networking ]**
- help
    > docker network --help
- show networks
    > docker network ls
- inspect a network
    > docker network inspect [network]
- create a network
    > docker network create --driver
- attach a network to a container
    > docker network connect
- detach a network from a container
    > docker network disconnect

- there are 3 networks that are by default created for docker (docker network ls).
    - bridge : the default network to which all our containers get attached to.
    - host   : special network which skips dockers virtual networking and directly attaches containers to the host network.
    - none   :

- create your own network (default [--driver] = bridge)
    > docker network create my_app_net
- run a container in a custom network
    > docker container run -d --name new_nginx --network my_app_net nginx
- to connect an existing container with a new network
    > docker network connect [network.id] [container.id]
- to disconnect a container from a new network
    > docker network disconnect [network.id] [container.id]
* single container can be connected to multiple networks
    > docker container inspect [container.id]

### Docker networks: DNS ###
* we can't rely on ip addresses because they are dynamic.
* [--link] option to enable DNS on default bridge network
* when containers talk to each other within a network, docker uses container names.

- lets have two containers running within a custom network (when containers belongs to the default bridge network they can't talk to each other because DNS server is not installed in that)
    > docker container run -d --name webhost --network my_net nginx
    > docker container run -d --name nginx --network my_net nginx
- get into both the containers and install ping [apt install iputils-ping] after [apt update]
    > docker container exec -it webhost ping nginx

### DNS round robin test ###
- [--net-alias] : create multiple names to point a single container. we can even assign the same alias to different containers. thus overcome the limitation of not having duplicate named container.

- create a new network
    > docker network create winter
- create 2 elasticsearch containers having same alias names
    > docker container run -d --net winter --net-alias search elasticsearch:2
    > docker container run -d --net winter --net-alias search elasticsearch:2
- elasticsearch listens to port 9200 for search result. by default it prepares dummy json search result.
- we are going to see that dummy json search result coming from either of the containers.

- run alpine container to test dns-lookup (nslookup) for name "search"
    > docker container run --rm --net winter alpine nslookup search
- run centos container to check dns-round-robin-name-lookup using curl (since curl is default in centos).
    > docker container run --rm --net winter centos curl -s search:9200

### Docker Images ###
> docker image ls

- pull specific version of an image
    > docker pull nginx:1.11

- history of an image
    > docker history nginx:latest

* docker images are a collection of layers with a unique SHA. most images start with a base image (may be from a different project) and on top of that developers add their own layer. this approach saves time and space. each layer of a Docker image is viewable using dockers [history] command. downloaded layers are cached, so when we pull an image from the docker hub, we only download the layers required for the particular image.
* docker images are read-only. but when we run a container based on an image, that container may change a file which is actually part of the docker image, this is known as copy-on-write (cow). the running container will actually copy the file into itself and change it inside the container (not the actual file from the image).

- inspect an image
    - Exposed ports of the image (so better configure those with host ports)
    - Cmd that the image runs when its run in default mode
    - Env variables
    > docker image inspect nginx

